{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network using pytorch\n",
    "\n",
    "This will go over my implementation of All-CNN-C model, introduced in paper [Striving For Simplicity: The All Convolutional Net](https://arxiv.org/abs/1412.6806), using pytorch library\n",
    "\n",
    "In usual CNN, 3 types of layers are used\n",
    "- Convolution Layer\n",
    "- Pooling Layer\n",
    "- Fully Connected Layer\n",
    "\n",
    "This paper present All-CNN-C convolutional network, which utilizes \n",
    "- **Convolution with 2 strides** instead of MaxPool\n",
    "- **Global Averaging and softmax** instead of Fully Connected Layer\n",
    "\n",
    "## Architecture \n",
    "\n",
    "- First Conv Layers\n",
    "    * 3 x 3 Conv, ReLU 96, Stride 1\n",
    "    * 3 x 3 Conv, ReLU 96, Stride 1\n",
    "- First Conv Pooling Layer\n",
    "    * 3 x 3 Conv, ReLU 96, stride 2\n",
    "\n",
    "| Layer | Kernel | Stride | Image Size | \n",
    "|-------|--------|--------|\n",
    "|  Conv, ReLU 96 | 3 x 3  | 1 x 1  | 32 x 32 | \n",
    "|  Conv, ReLU 96 | 3 x 3  | 1 x 1  | \n",
    "|  **Conv, ReLU 96** | **3 x 3**  | **2 x 2**  |\n",
    "|  Conv, ReLU 192 | 3 x 3  | 1 x 1  |\n",
    "|  Conv, ReLU 192 | 3 x 3  | 1 x 1  |\n",
    "|  **Conv, ReLU 192** | **3 x 3**  | **2 x 2**  |\n",
    "|  Conv, ReLU 192 | 3 x 3  | 1 x 1  |\n",
    "|  Conv, ReLU 192 | 1 x 1  | 1 x 1  | \n",
    "|  Conv, ReLU 10 | 1 x 1  | 1 x 1  | 6 x 6 |\n",
    "| **Global Average** |  **6 x 6**  | **1 x 1** |\n",
    "|           10 Way Softmax          |\n",
    "\n",
    "\n",
    "- Batch Normalization was applied for each layers, except the first Conv layer\n",
    "\n",
    "## Creating the Model using Pytorch LIbrary\n",
    "\n",
    "### Importing neccesary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modulizing the model \n",
    "- Here, we will break down the models into smaller 'modules', which contains...\n",
    "    * 2D convolution\n",
    "    * Batch Normalization\n",
    "    * LeakyReLU activation\n",
    "We'll do this by creating pytorch class, called CUnit, and each has\n",
    "- \\__init\\__: this initializes neccesary layers\n",
    "- forward: perform the calculations using defined layers in  \\__init\\__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUnit(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, batch_norm=True):\n",
    "        super(CUnit, self).__init__()\n",
    "        pass\n",
    "  \n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "    def forward(self, inp, batch_norm=True):\n",
    "        out = self.conv(inp)\n",
    "        if batch_norm:\n",
    "            out = self.bn(out)\n",
    "        out = self.lrelu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes for CUnit class\n",
    "* \\__init\\__()\n",
    "    - in_channels: depth/channels of input images to the unit\n",
    "    - out_channels: depth/channels of output images from the unit\n",
    "    - kernel_size: kernel/filter size of the convolution\n",
    "    - stride: how many pixels the filter moves in one step of convolution\n",
    "    - padding: padding on input images\n",
    "    - batch_norm: whether or not to apply batch normalization in the unit\n",
    "    \n",
    "* forward()\n",
    "    - inp: input (image matrix)\n",
    "    - batch_norm: Boolean value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the whole model\n",
    "\n",
    "**Now that we constructed the unit class, let's use them to construct the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class all_CNN(nn.Module):\n",
    "    def __init__(self, image_depth, num_classes):\n",
    "        # first, set up parameters and configs\n",
    "        self.image_depth = image_depth\n",
    "        self.num_classes = num_classes\n",
    "        self.num_out1 = 96\n",
    "        self.num_out2 = 64\n",
    "        # Defining dropouts with defined probability\n",
    "        self.drop1 = nn.Dropout(p=0.2)\n",
    "        self.drop2 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # now we create units using the CUnit class, based on the\n",
    "        # model table above...\n",
    "        self.conv1 = CUnit(in_channels=self.image_depth, out_channels=96, stride=1, batch_norm=False)\n",
    "        self.conv2 = CUnit(in_channels=96, out_channels=96)\n",
    "        # here, we'll use 2 stride convolution layer instead of pooling layer\n",
    "        self.convPool1 = CUnit(in_channels=96, out_channels=96, stride=2, padding=0) \n",
    "        self.conv3 = CUnit(in_channels=96, out_channels=192)\n",
    "        self.conv4 = CUnit(in_channels=192, out_channels=192)\n",
    "        # Second ConvPool Layer\n",
    "        self.convPool2 = CUnit(in_channels=192, out_channels=192, stride=2)\n",
    "        self.conv5 = CUnit(in_channels=192, out_channels=192, padding=0)\n",
    "        self.conv6 = CUnit(in_channels=192, out_channels=192, kernel_size=1, padding=0)\n",
    "        self.conv7 = CUnit(in_channels=192, out_channels=self.num_classes, kernel_size=1, padding=0)\n",
    "        \n",
    "        # Average Pooling and softmax layers \n",
    "        self.avp = nn.AvgPool2d(6)\n",
    "        self.softmax = nn.softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolution and convPool computations\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.convPool1(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.convPool2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        # average pooling\n",
    "        avg = self.avp(x)\n",
    "        # changing shape\n",
    "        avg = avg.view(-1, self.num_classes)\n",
    "        # applying softmax\n",
    "        out = self.softmax(avg)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Model Training\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "Here, we will use image dataset, CIFAR10. Image datasets for classification.\n",
    "- [CIFAR 10 & 100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "- 32 x 32 pixel images of 10 classes\n",
    "- 60000 images total, 10000 for testing, 50000 for training.\n",
    "- 6000 images per class\n",
    "\n",
    "**Preprocessing**\n",
    "- Horizontal Flip\n",
    "- Normalization\n",
    "are used for the data.\n",
    "\n",
    "**Note**: the code is designed so that it will take advantage of GPU if it is available.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stuff...\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from all_CNN_model import all_CNN\n",
    "from logger import Logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported Modules\n",
    "- `os`: used to execute system command in python\n",
    "- `torch`: that's the ML library... duh\n",
    "- `torchvision`: importing CIFAR10. Other major datasets are also available through this.\n",
    "- `torch.nn`: for layers, activations\n",
    "- `MultiStepLR`: for adaptive learning rate, refer the paper\n",
    "- `transforms`: preprocessing CIFAR10\n",
    "- `save_image`: Saving image\n",
    "- `all_CNN`: our model\n",
    "- `Logger`: custom logger class, logging training data using tensorflow. thanks to someone from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the device, CPU or GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper Parameters\n",
    "lr = 0.04 #0.25, 0.01, 0.05, ,  \n",
    "image_size = 32# for image's hight and witdh\n",
    "num_epochs = 50 # how many times to go through the training set\n",
    "num_classes = 10\n",
    "batch_size = 64 \n",
    "image_depth = 3 # or channels\n",
    "sample_dir = 'CIFAR10_sample'\n",
    "\n",
    "# Create a directory\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "\n",
    "# Initializing the logger\n",
    "logPath = 'logs_CNN/'\n",
    "record_name = 'CIFAR10_' + str(lr)\n",
    "logger = Logger(logPath + record_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Little bit about torch.device...\n",
    "\n",
    "`torch.cuda.is_available()` returns if cuda is available. \n",
    "`torch.device` create device, that can be used later for Variable/tensor setting.\n",
    "\n",
    "This is done, so we don't have to rewrite the whole program for cuda and cpu option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "- `transforms.Compose()` accepts list of transformation and define transformation to apply. Here we'll use horizontal Flip and Normalize\n",
    "- `transform.RandomHorizontalFlip(p=n)` flips the image horizontally, with probability of n\n",
    "- `transform.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))` normalize the images, with provided mean and standard deviation values. One value for each channel, here 3 channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([ # transforms.Compose, list of transforms to perform\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "### Loading the dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='CIFAR10_data/', # where at??\n",
    "                                   train=True,\n",
    "                                   transform=transform, # pass the transform  we made\n",
    "                                   download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='CIFAR10_data/', # where at??\n",
    "                                   train=False,\n",
    "                                   transform=transform, # pass the transform  we made\n",
    "                                   download=True)\n",
    "\n",
    "### Data Loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True, drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Loaders\n",
    "We first load the dataset for train and test, and create dataloader for each of them.\n",
    "\n",
    "**Dataset**\n",
    "- `torchvision.datasets.DATASET_NAME()` load the dataset with the name. \n",
    "    * **root**: relative file path to store the data\n",
    "    * **train**: bol, train set or not\n",
    "    * **trainsform**: accept the pre-defined transformation. Here we provide the one we created \n",
    "    * **download**: if we download the dataset\n",
    " \n",
    "**Data Loaders**\n",
    "- `torch.utils.data.DataLoader()` create data loader, which provide batches of the data to the model during training\n",
    "    * **dataset**: dataset to create the loader from\n",
    "    * **batch_size**: batch size\n",
    "    * **shuffle**: whether to shuffle data\n",
    "    * **drop_last**: drop the last data points in the loader which is smaller than the batch size, if it's true\n",
    "    \n",
    "\n",
    "### Model and Training Setups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model with parameters\n",
    "D = all_CNN(image_depth, num_classes)\n",
    "\n",
    "# Device setting\n",
    "# D.to() moves and/or casts the parameters and buffers to device(cuda), dtype\n",
    "# setting to whatevefr the device set earlier\n",
    "D = D.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(D.parameters(), lr=lr, momentum=0.9, weight_decay=0.001)\n",
    "\n",
    "## Adaptive Learning Rate ##\n",
    "scheduler = MultiStepLR(optimizer, milestones=[200, 250, 300], gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `model/variable/tensor.to(device)` casts/move the parameters and buffers to device(cpu/cuda) or dtypes\n",
    "- `MultiStepLR`: adaptive learning rate scheduler\n",
    "    * `milestones`: specifies the epoch number to change the lr\n",
    "    * `gamma`: $lr_{new} = gamma*lr_{old}$\n",
    "    \n",
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denormalize the image\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)\n",
    "\n",
    "# evaluate the model\n",
    "def evaluate(mode, num):\n",
    "    '''\n",
    "    Evaluate using only first num batches from loader\n",
    "    '''\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # define the mode, to use training set or testing set\n",
    "    if mode == 'train':\n",
    "        loader = train_loader\n",
    "    elif mode == 'test':\n",
    "        loader = test_loader\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(loader):\n",
    "            # create the variables for image and target\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            # forward pass\n",
    "            output = D(data)\n",
    "            # calculate, and add the loss of the batch to total loss\n",
    "            test_loss += criterion(output, target).item()\n",
    "            # make prediction, and get the index numbers as class label\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            # compare prediction with the target\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "            if i % 10 == 0:\n",
    "                print(i)\n",
    "            if i == num: # break out when numth number of batch\n",
    "                break\n",
    "        sample_size = batch_size * num # How many datapoints\n",
    "        test_loss /= sample_size # average loss\n",
    "        print('\\n' + mode + 'set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, sample_size,\n",
    "            100. * correct / sample_size)) # acccuracy\n",
    "    return 100. * correct / sample_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- `Variable.item()` returns the data, if Variable is 1D\n",
    "- `Variable.data` returns Tensor\n",
    "- `Tensor.max(dim, keepdim)` returns max values along the dim, and set keepdim=True, to retain the shape of the original tensor\n",
    "\n",
    "\n",
    "### Train the Model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
