{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network using pytorch\n",
    "\n",
    "This will go over my implementation of All-CNN-C model, introduced in paper [Striving For Simplicity: The All Convolutional Net](https://arxiv.org/abs/1412.6806), using pytorch library\n",
    "\n",
    "In usual CNN, 3 types of layers are used\n",
    "- Convolution Layer\n",
    "- Pooling Layer\n",
    "- Fully Connected Layer\n",
    "\n",
    "This paper present All-CNN-C convolutional network, which utilizes \n",
    "- **Convolution with 2 strides** instead of MaxPool\n",
    "- **Global Averaging and softmax** instead of Fully Connected Layer\n",
    "\n",
    "## Architecture \n",
    "\n",
    "- First Conv Layers\n",
    "    * 3 x 3 Conv, ReLU 96, Stride 1\n",
    "    * 3 x 3 Conv, ReLU 96, Stride 1\n",
    "- First Conv Pooling Layer\n",
    "    * 3 x 3 Conv, ReLU 96, stride 2\n",
    "\n",
    "| Layer | Kernel | Stride | Image Size | \n",
    "|-------|--------|--------|\n",
    "|  Conv, ReLU 96 | 3 x 3  | 1 x 1  | 32 x 32 | \n",
    "|  Conv, ReLU 96 | 3 x 3  | 1 x 1  | \n",
    "|  **Conv, ReLU 96** | **3 x 3**  | **2 x 2**  |\n",
    "|  Conv, ReLU 192 | 3 x 3  | 1 x 1  |\n",
    "|  Conv, ReLU 192 | 3 x 3  | 1 x 1  |\n",
    "|  **Conv, ReLU 192** | **3 x 3**  | **2 x 2**  |\n",
    "|  Conv, ReLU 192 | 3 x 3  | 1 x 1  |\n",
    "|  Conv, ReLU 192 | 1 x 1  | 1 x 1  | \n",
    "|  Conv, ReLU 10 | 1 x 1  | 1 x 1  | 6 x 6 |\n",
    "| **Global Average** |  **6 x 6**  | **1 x 1** |\n",
    "|           10 Way Softmax          |\n",
    "\n",
    "\n",
    "- Batch Normalization was applied for each layers, except the first Conv layer\n",
    "\n",
    "## Creating the Model using Pytorch LIbrary\n",
    "\n",
    "### Importing neccesary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modulizing the model \n",
    "- Here, we will break down the models into smaller 'modules', which contains...\n",
    "    * 2D convolution\n",
    "    * Batch Normalization\n",
    "    * LeakyReLU activation\n",
    "We'll do this by creating pytorch class, called CUnit, and each has\n",
    "- \\__init\\__: this initializes neccesary layers\n",
    "- forward: perform the calculations using defined layers in  \\__init\\__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUnit(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, batch_norm=True):\n",
    "        super(CUnit, self).__init__()\n",
    "        pass\n",
    "  \n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "    def forward(self, inp, batch_norm=True):\n",
    "        out = self.conv(inp)\n",
    "        if batch_norm:\n",
    "            out = self.bn(out)\n",
    "        out = self.lrelu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes for CUnit class\n",
    "* \\__init\\__()\n",
    "    - in_channels: depth/channels of input images to the unit\n",
    "    - out_channels: depth/channels of output images from the unit\n",
    "    - kernel_size: kernel/filter size of the convolution\n",
    "    - stride: how many pixels the filter moves in one step of convolution\n",
    "    - padding: padding on input images\n",
    "    - batch_norm: whether or not to apply batch normalization in the unit\n",
    "    \n",
    "* forward()\n",
    "    - inp: input (image matrix)\n",
    "    - batch_norm: Boolean value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the whole model\n",
    "\n",
    "**Now that we constructed the unit class, let's use them to construct the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class all_CNN(nn.Module):\n",
    "    def __init__(self, image_depth, num_classes):\n",
    "        # first, set up parameters and configs\n",
    "        self.image_depth = image_depth\n",
    "        self.num_classes = num_classes\n",
    "        self.num_out1 = 96\n",
    "        self.num_out2 = 64\n",
    "        # Defining dropouts with defined probability\n",
    "        self.drop1 = nn.Dropout(p=0.2)\n",
    "        self.drop2 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # now we create units using the CUnit class, based on the\n",
    "        # model table above...\n",
    "        self.conv1 = CUnit(in_channels=self.image_depth, out_channels=96, stride=1, batch_norm=False)\n",
    "        self.conv2 = CUnit(in_channels=96, out_channels=96)\n",
    "        # here, we'll use 2 stride convolution layer instead of pooling layer\n",
    "        self.convPool1 = CUnit(in_channels=96, out_channels=96, stride=2, padding=0) \n",
    "        self.conv3 = CUnit(in_channels=96, out_channels=192)\n",
    "        self.conv4 = CUnit(in_channels=192, out_channels=192)\n",
    "        # Second ConvPool Layer\n",
    "        self.convPool2 = CUnit(in_channels=192, out_channels=192, stride=2)\n",
    "        self.conv5 = CUnit(in_channels=192, out_channels=192, padding=0)\n",
    "        self.conv6 = CUnit(in_channels=192, out_channels=192, kernel_size=1, padding=0)\n",
    "        self.conv7 = CUnit(in_channels=192, out_channels=self.num_classes, kernel_size=1, padding=0)\n",
    "        \n",
    "        # Average Pooling and softmax layers \n",
    "        self.avp = nn.AvgPool2d(6)\n",
    "        self.softmax = nn.softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolution and convPool computations\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.convPool1(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.convPool2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        # average pooling\n",
    "        avg = self.avp(x)\n",
    "        # changing shape\n",
    "        avg = avg.view(-1, self.num_classes)\n",
    "        # applying softmax\n",
    "        out = self.softmax(avg)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Model Training\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "Here, we will use image dataset, CIFAR10. Image datasets for classification.\n",
    "- [CIFAR 10 & 100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "- 32 x 32 pixel images of 10 classes\n",
    "- 60000 images total, 10000 for testing, 50000 for training.\n",
    "- 6000 images per class\n",
    "\n",
    "**Preprocessing**\n",
    "- Horizontal Flip\n",
    "- Normalization\n",
    "are used for the data.\n",
    "\n",
    "**Note**: the code is designed so that it will take advantage of GPU if it is available.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stuff...\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from all_CNN_model import all_CNN\n",
    "from logger import Logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported Modules\n",
    "- `os`: used to execute system command in python\n",
    "- `torch`: that's the ML library\n",
    "- `torchvision`: importing CIFAR10. Other major datasets are also available through this.\n",
    "- `torch.nn`: for layers, activations\n",
    "- `MultiStepLR`: for adaptive learning rate, refer the paper\n",
    "- `transforms`: preprocessing CIFAR10\n",
    "- `save_image`: Saving image\n",
    "- `all_CNN`: our model\n",
    "- `Logger`: custom logger class, logging training data using tensorflow. thanks to someone from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the device, CPU or GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper Parameters\n",
    "lr = 0.04 #0.25, 0.01, 0.05, ,  \n",
    "image_size = 32# for image's hight and witdh\n",
    "num_epochs = 50 # how many times to go through the training set\n",
    "num_classes = 10\n",
    "batch_size = 64 \n",
    "image_depth = 3 # or channels\n",
    "sample_dir = 'CIFAR10_sample'\n",
    "\n",
    "# Create a directory\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "\n",
    "# Initializing the logger\n",
    "logPath = 'logs_CNN/'\n",
    "record_name = 'CIFAR10_' + str(lr)\n",
    "logger = Logger(logPath + record_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Little bit about torch.device...\n",
    "\n",
    "`torch.cuda.is_available()` returns if cuda is available. \n",
    "`torch.device` create device, that can be used later for Variable/tensor setting.\n",
    "\n",
    "This is done, so we don't have to rewrite the whole program for cuda and cpu option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "- `transforms.Compose()` accepts list of transformation and define transformation to apply. Here we'll use horizontal Flip and Normalize\n",
    "- `transform.RandomHorizontalFlip(p=n)` flips the image horizontally, with probability of n\n",
    "- `transform.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))` normalize the images, with provided mean and standard deviation values. One value for each channel, here 3 channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([ # transforms.Compose, list of transforms to perform\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "### Loading the dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='CIFAR10_data/', # where at??\n",
    "                                   train=True,\n",
    "                                   transform=transform, # pass the transform  we made\n",
    "                                   download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='CIFAR10_data/', # where at??\n",
    "                                   train=False,\n",
    "                                   transform=transform, # pass the transform  we made\n",
    "                                   download=True)\n",
    "\n",
    "### Data Loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True, drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Loaders\n",
    "We first load the dataset for train and test, and create dataloader for each of them.\n",
    "\n",
    "**Dataset**\n",
    "- `torchvision.datasets.DATASET_NAME()` load the dataset with the name. \n",
    "    * **root**: relative file path to store the data\n",
    "    * **train**: bol, train set or not\n",
    "    * **trainsform**: accept the pre-defined transformation. Here we provide the one we created \n",
    "    * **download**: if we download the dataset\n",
    " \n",
    "**Data Loaders**\n",
    "- `torch.utils.data.DataLoader()` create data loader, which provide batches of the data to the model during training\n",
    "    * **dataset**: dataset to create the loader from\n",
    "    * **batch_size**: batch size\n",
    "    * **shuffle**: whether to shuffle data\n",
    "    * **drop_last**: drop the last data points in the loader which is smaller than the batch size, if it's true\n",
    "    \n",
    "\n",
    "### Model and Training Setups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model with parameters\n",
    "D = all_CNN(image_depth, num_classes)\n",
    "\n",
    "# Device setting\n",
    "# D.to() moves and/or casts the parameters and buffers to device(cuda), dtype\n",
    "# setting to whatevefr the device set earlier\n",
    "D = D.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(D.parameters(), lr=lr, momentum=0.9, weight_decay=0.001)\n",
    "\n",
    "## Adaptive Learning Rate ##\n",
    "scheduler = MultiStepLR(optimizer, milestones=[200, 250, 300], gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `model/variable/tensor.to(device)` casts/move the parameters and buffers to device(cpu/cuda) or dtypes\n",
    "- `MultiStepLR`: adaptive learning rate scheduler\n",
    "    * `milestones`: specifies the epoch number to change the lr\n",
    "    * `gamma`: $lr_{new} = gamma*lr_{old}$\n",
    "    \n",
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denormalize the image\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)\n",
    "\n",
    "# evaluate the model\n",
    "def evaluate(mode, num):\n",
    "    '''\n",
    "    Evaluate using only first num batches from loader\n",
    "    '''\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # define the mode, to use training set or testing set\n",
    "    if mode == 'train':\n",
    "        loader = train_loader\n",
    "    elif mode == 'test':\n",
    "        loader = test_loader\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(loader):\n",
    "            # create the variables for image and target\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            # forward pass\n",
    "            output = D(data)\n",
    "            # calculate, and add the loss of the batch to total loss\n",
    "            test_loss += criterion(output, target).item()\n",
    "            # make prediction, and get the index numbers as class label\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            # compare prediction with the target\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "            if i % 10 == 0:\n",
    "                print(i)\n",
    "            if i == num: # break out when numth number of batch\n",
    "                break\n",
    "        sample_size = batch_size * num # How many datapoints\n",
    "        test_loss /= sample_size # average loss\n",
    "        print('\\n' + mode + 'set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, sample_size,\n",
    "            100. * correct / sample_size)) # acccuracy\n",
    "    return 100. * correct / sample_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- `Variable.item()` returns the data, if Variable is 1D\n",
    "- `Variable.data` returns Tensor\n",
    "- `Tensor.max(dim, keepdim)` returns max values along the dim, and set keepdim=True, to retain the shape of the original tensor\n",
    "\n",
    "\n",
    "### Train the Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Step [2/781], loss: 2.3042\n",
      "i+1 2  Lr: 0.04\n",
      "Epoch [0/50], Step [4/781], loss: 2.3021\n",
      "i+1 4  Lr: 0.04\n",
      "Epoch [0/50], Step [6/781], loss: 2.2986\n",
      "i+1 6  Lr: 0.04\n",
      "Epoch [0/50], Step [8/781], loss: 2.2955\n",
      "i+1 8  Lr: 0.04\n",
      "Epoch [0/50], Step [10/781], loss: 2.2927\n",
      "i+1 10  Lr: 0.04\n",
      "Epoch [0/50], Step [12/781], loss: 2.2887\n",
      "i+1 12  Lr: 0.04\n",
      "Epoch [0/50], Step [14/781], loss: 2.2834\n",
      "i+1 14  Lr: 0.04\n",
      "Epoch [0/50], Step [16/781], loss: 2.2805\n",
      "i+1 16  Lr: 0.04\n",
      "Epoch [0/50], Step [18/781], loss: 2.2912\n",
      "i+1 18  Lr: 0.04\n",
      "Epoch [0/50], Step [20/781], loss: 2.2854\n",
      "i+1 20  Lr: 0.04\n",
      "Epoch [0/50], Step [22/781], loss: 2.2801\n",
      "i+1 22  Lr: 0.04\n",
      "Epoch [0/50], Step [24/781], loss: 2.2824\n",
      "i+1 24  Lr: 0.04\n",
      "Epoch [0/50], Step [26/781], loss: 2.2768\n",
      "i+1 26  Lr: 0.04\n",
      "Epoch [0/50], Step [28/781], loss: 2.2981\n",
      "i+1 28  Lr: 0.04\n",
      "Epoch [0/50], Step [30/781], loss: 2.2878\n",
      "i+1 30  Lr: 0.04\n",
      "Epoch [0/50], Step [32/781], loss: 2.2801\n",
      "i+1 32  Lr: 0.04\n",
      "Epoch [0/50], Step [34/781], loss: 2.2725\n",
      "i+1 34  Lr: 0.04\n",
      "Epoch [0/50], Step [36/781], loss: 2.2806\n",
      "i+1 36  Lr: 0.04\n",
      "Epoch [0/50], Step [38/781], loss: 2.2723\n",
      "i+1 38  Lr: 0.04\n",
      "Epoch [0/50], Step [40/781], loss: 2.2804\n",
      "i+1 40  Lr: 0.04\n",
      "Epoch [0/50], Step [42/781], loss: 2.2619\n",
      "i+1 42  Lr: 0.04\n",
      "Epoch [0/50], Step [44/781], loss: 2.2778\n",
      "i+1 44  Lr: 0.04\n",
      "Epoch [0/50], Step [46/781], loss: 2.2762\n",
      "i+1 46  Lr: 0.04\n",
      "Epoch [0/50], Step [48/781], loss: 2.2820\n",
      "i+1 48  Lr: 0.04\n",
      "Epoch [0/50], Step [50/781], loss: 2.2713\n",
      "i+1 50  Lr: 0.04\n",
      "Epoch [0/50], Step [52/781], loss: 2.2822\n",
      "i+1 52  Lr: 0.04\n",
      "Epoch [0/50], Step [54/781], loss: 2.2612\n",
      "i+1 54  Lr: 0.04\n",
      "Epoch [0/50], Step [56/781], loss: 2.2542\n",
      "i+1 56  Lr: 0.04\n",
      "Epoch [0/50], Step [58/781], loss: 2.2495\n",
      "i+1 58  Lr: 0.04\n",
      "Epoch [0/50], Step [60/781], loss: 2.2538\n",
      "i+1 60  Lr: 0.04\n",
      "Epoch [0/50], Step [62/781], loss: 2.2511\n",
      "i+1 62  Lr: 0.04\n",
      "Epoch [0/50], Step [64/781], loss: 2.2685\n",
      "i+1 64  Lr: 0.04\n",
      "Epoch [0/50], Step [66/781], loss: 2.2567\n",
      "i+1 66  Lr: 0.04\n",
      "Epoch [0/50], Step [68/781], loss: 2.2535\n",
      "i+1 68  Lr: 0.04\n",
      "Epoch [0/50], Step [70/781], loss: 2.2518\n",
      "i+1 70  Lr: 0.04\n",
      "Epoch [0/50], Step [72/781], loss: 2.2568\n",
      "i+1 72  Lr: 0.04\n",
      "Epoch [0/50], Step [74/781], loss: 2.2568\n",
      "i+1 74  Lr: 0.04\n",
      "Epoch [0/50], Step [76/781], loss: 2.2583\n",
      "i+1 76  Lr: 0.04\n",
      "Epoch [0/50], Step [78/781], loss: 2.2618\n",
      "i+1 78  Lr: 0.04\n",
      "Epoch [0/50], Step [80/781], loss: 2.2152\n",
      "i+1 80  Lr: 0.04\n",
      "Epoch [0/50], Step [82/781], loss: 2.2613\n",
      "i+1 82  Lr: 0.04\n",
      "Epoch [0/50], Step [84/781], loss: 2.2762\n",
      "i+1 84  Lr: 0.04\n",
      "Epoch [0/50], Step [86/781], loss: 2.2703\n",
      "i+1 86  Lr: 0.04\n",
      "Epoch [0/50], Step [88/781], loss: 2.2678\n",
      "i+1 88  Lr: 0.04\n",
      "Epoch [0/50], Step [90/781], loss: 2.2434\n",
      "i+1 90  Lr: 0.04\n",
      "Epoch [0/50], Step [92/781], loss: 2.2237\n",
      "i+1 92  Lr: 0.04\n",
      "Epoch [0/50], Step [94/781], loss: 2.2548\n",
      "i+1 94  Lr: 0.04\n",
      "Epoch [0/50], Step [96/781], loss: 2.2360\n",
      "i+1 96  Lr: 0.04\n",
      "Epoch [0/50], Step [98/781], loss: 2.2447\n",
      "i+1 98  Lr: 0.04\n",
      "Epoch [0/50], Step [100/781], loss: 2.2780\n",
      "i+1 100  Lr: 0.04\n",
      "Epoch [0/50], Step [102/781], loss: 2.2325\n",
      "i+1 102  Lr: 0.04\n",
      "Epoch [0/50], Step [104/781], loss: 2.2421\n",
      "i+1 104  Lr: 0.04\n",
      "Epoch [0/50], Step [106/781], loss: 2.2159\n",
      "i+1 106  Lr: 0.04\n",
      "Epoch [0/50], Step [108/781], loss: 2.2276\n",
      "i+1 108  Lr: 0.04\n",
      "Epoch [0/50], Step [110/781], loss: 2.2238\n",
      "i+1 110  Lr: 0.04\n",
      "Epoch [0/50], Step [112/781], loss: 2.2605\n",
      "i+1 112  Lr: 0.04\n",
      "Epoch [0/50], Step [114/781], loss: 2.2291\n",
      "i+1 114  Lr: 0.04\n",
      "Epoch [0/50], Step [116/781], loss: 2.2338\n",
      "i+1 116  Lr: 0.04\n",
      "Epoch [0/50], Step [118/781], loss: 2.2335\n",
      "i+1 118  Lr: 0.04\n",
      "Epoch [0/50], Step [120/781], loss: 2.2002\n",
      "i+1 120  Lr: 0.04\n",
      "Epoch [0/50], Step [122/781], loss: 2.2539\n",
      "i+1 122  Lr: 0.04\n",
      "Epoch [0/50], Step [124/781], loss: 2.2164\n",
      "i+1 124  Lr: 0.04\n",
      "Epoch [0/50], Step [126/781], loss: 2.2420\n",
      "i+1 126  Lr: 0.04\n",
      "Epoch [0/50], Step [128/781], loss: 2.2415\n",
      "i+1 128  Lr: 0.04\n",
      "Epoch [0/50], Step [130/781], loss: 2.2138\n",
      "i+1 130  Lr: 0.04\n",
      "Epoch [0/50], Step [132/781], loss: 2.2219\n",
      "i+1 132  Lr: 0.04\n",
      "Epoch [0/50], Step [134/781], loss: 2.2178\n",
      "i+1 134  Lr: 0.04\n",
      "Epoch [0/50], Step [136/781], loss: 2.2774\n",
      "i+1 136  Lr: 0.04\n",
      "Epoch [0/50], Step [138/781], loss: 2.1967\n",
      "i+1 138  Lr: 0.04\n",
      "Epoch [0/50], Step [140/781], loss: 2.2494\n",
      "i+1 140  Lr: 0.04\n",
      "Epoch [0/50], Step [142/781], loss: 2.2477\n",
      "i+1 142  Lr: 0.04\n",
      "Epoch [0/50], Step [144/781], loss: 2.2271\n",
      "i+1 144  Lr: 0.04\n",
      "Epoch [0/50], Step [146/781], loss: 2.2045\n",
      "i+1 146  Lr: 0.04\n",
      "Epoch [0/50], Step [148/781], loss: 2.2392\n",
      "i+1 148  Lr: 0.04\n",
      "Epoch [0/50], Step [150/781], loss: 2.2199\n",
      "i+1 150  Lr: 0.04\n",
      "Epoch [0/50], Step [152/781], loss: 2.2238\n",
      "i+1 152  Lr: 0.04\n",
      "Epoch [0/50], Step [154/781], loss: 2.2056\n",
      "i+1 154  Lr: 0.04\n",
      "Epoch [0/50], Step [156/781], loss: 2.2157\n",
      "i+1 156  Lr: 0.04\n",
      "Epoch [0/50], Step [158/781], loss: 2.1688\n",
      "i+1 158  Lr: 0.04\n",
      "Epoch [0/50], Step [160/781], loss: 2.2097\n",
      "i+1 160  Lr: 0.04\n",
      "Epoch [0/50], Step [162/781], loss: 2.2537\n",
      "i+1 162  Lr: 0.04\n",
      "Epoch [0/50], Step [164/781], loss: 2.2147\n",
      "i+1 164  Lr: 0.04\n",
      "Epoch [0/50], Step [166/781], loss: 2.2225\n",
      "i+1 166  Lr: 0.04\n",
      "Epoch [0/50], Step [168/781], loss: 2.2171\n",
      "i+1 168  Lr: 0.04\n",
      "Epoch [0/50], Step [170/781], loss: 2.2507\n",
      "i+1 170  Lr: 0.04\n",
      "Epoch [0/50], Step [172/781], loss: 2.1906\n",
      "i+1 172  Lr: 0.04\n",
      "Epoch [0/50], Step [174/781], loss: 2.1886\n",
      "i+1 174  Lr: 0.04\n",
      "Epoch [0/50], Step [176/781], loss: 2.2085\n",
      "i+1 176  Lr: 0.04\n",
      "Epoch [0/50], Step [178/781], loss: 2.2002\n",
      "i+1 178  Lr: 0.04\n",
      "Epoch [0/50], Step [180/781], loss: 2.2453\n",
      "i+1 180  Lr: 0.04\n",
      "Epoch [0/50], Step [182/781], loss: 2.2087\n",
      "i+1 182  Lr: 0.04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-73d1a56f3a1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reset the grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update the parameters (weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### Start Training ####\n",
    "# num of batches in the total dataset, here 60000/100\n",
    "total_step = len(train_loader) \n",
    "\n",
    "count = 0\n",
    "for epoch in range(num_epochs): # How many times to go through the dataset\n",
    "    D.train()\n",
    "    for i, (images, labels) in enumerate(train_loader): # each batch\n",
    "        count += 1\n",
    "        # reshape the data, forward pass, and calculate the loss\n",
    "        images = images.reshape(batch_size, image_depth, image_size, image_size).to(device) # reshape and set to cuda/cpu\n",
    "        outputs = D(images) # using real data\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backpropagation\n",
    "        optimizer.zero_grad() # reset the grad\n",
    "        loss.backward() # backprop\n",
    "        optimizer.step() # update the parameters (weights)\n",
    "        \n",
    "        # Printing the training info\n",
    "        if (i+1) % 2 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], loss: {:.4f}'.format(epoch, num_epochs, i+1, total_step, loss.item()))\n",
    "            print('i+1', i+1, ' Lr:', lr)\n",
    "            \n",
    "        \n",
    "        ### Tensorboard Logging ###\n",
    "        if i % 10 == 0:\n",
    "            # 1. Log scalar values (scalar summary)\n",
    "            info = {'loss':loss.item()}\n",
    "\n",
    "            for tag, value in info.items():\n",
    "                logger.scalar_summary(tag, value, count+1)\n",
    "\n",
    "\n",
    "            # 2. Log values and gradients of the parameters (histogram summary)\n",
    "            for tag, value in D.named_parameters():\n",
    "                tag = tag.replace('.', '/')\n",
    "                logger.histo_summary(tag, value.data.cpu().numpy(), count+1)\n",
    "                logger.histo_summary(tag+'/grad', value.grad.data.cpu().numpy(), i+1)\n",
    "\n",
    "            print('logging on tensorboard...', i)\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('calculating accuracy....')\n",
    "            ## Call the evaluate funtion, to get train and test accuracy\n",
    "            train_acc = evaluate('train', 50)\n",
    "            test_acc = evaluate('test', 50)\n",
    "\n",
    "            info = {'Train_acc':train_acc, 'Test_acc':test_acc}\n",
    "\n",
    "            for tag, value in info.items():\n",
    "                logger.scalar_summary(tag, value, count+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model checkpoint\n",
    "torch.save(D.state_dict(), os.path.join(sample_dir, 'D.ckpt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
